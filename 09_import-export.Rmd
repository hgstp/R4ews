# Daten I/O {#import-export}

```{r include = FALSE}
source("common.R")
```


## Überblick

Wir haben die Gapminder-Daten als tibble aus dem [gapminder] Paket geladen. Wir haben im letzten Abschnitt weder Daten noch abgeleitete Ergebnisse explizit in eine Datei geschrieben. Im wirklichen Leben wirst du aber ständig Daten, die in Tabellenform vorliegen, in R ein- und auslesen. Manchmal muss das sogar für Daten geschehen, die nicht in Tabellenform vorliegen.

Wie macht man das? Worauf muss man aufpassen?

### Daten Import

Für den Daten Import gibt es im Allgemeinen zwei Möglichkeiten:

* *"Überrasche mich!"* Diese Haltung musst du einnehmen, wenn du zum ersten Mal einen Datensatz erhältst. Du musst einfach froh, wenn du die Daten ohne Fehler importieren konntest. Dann schaust du dir das Ergebnis an,  entdeckst Fehler in den Daten und/oder beim Import. Du behebst sie und beginnst nochmal von vorne.
* *"Ein weiterer Tag im Paradies. "* Das ist die Einstellung, wenn du einen aufgeräumten Datensatz einliest, den du vorher in einem oder mehreren Reinigungsskripten wahnsinnig aufgeräumt haben. Es sollte keine Überraschungen geben. 

  
Im zweiten Fall, und im weiteren Verlauf des ersten Falles, lernst du tatsächlich eine Menge darüber, wie die Daten sind/sein sollten. Ein wichtiger Import-Ratschlag: **Verwende die Argumente der Importfunktion, um so weit wie möglich und so schnell wie möglich zu kommen**. Anfängercode hat oft eine Menge unnötigen nachträglichen Aufwand. Lese die Hilfe zu den Importfunktionen und nutzen die Argumente maximal aus, um den Import zu steuern.

### Daten Export

Es wird viele Gelegenheiten geben, bei denen du Daten aus R exportieren willst. Zwei wichtige Beispiele:

* einen gesäuberten Datensatz der bereit ist analysiert zu werden, den du heldenhaft aus recht unordentlichen Daten erstellt hast
* ein numerisches Ergebnis aus einer Datenaggregation oder Modellierung oder einer statistischen Schlussfolgerung 

Erster Tipp: __Der Output von heute ist der Input von morgen__. Denke an all die Schmerzen zurück, die du selbst beim Import von fremden Daten  erlitten hast, und fügen dir nicht selbst solche Schmerzen zu!

Zweiter Tipp: Sei nicht zu clever. Eine einfache Textdatei, die von einem Menschen in einem Texteditor lesbar ist, sollte dein Standard sein, bis du __einen guten Grund__ dafür hast, dass dies nicht funktionieren wird. Das Lesen und Schreiben in exotische Formate wird das erste sein, was in Zukunft oder auf einem anderen Computer kaputtgehen wird. Es schafft auch Barrieren für jeden, der ein anderes Toolkit hat als du. Strebe nach Zukunfts- und Idiotensicherheit.


Wie passt das zu unserer Betonung der dynamischen Berichterstattung über R Markdown? Es gibt für alles eine Zeit und einen Ort. Es gibt Projekte und Dokumente, bei denen du dich intensiv mit [knitr] und [rmarkdown]  beschäftigen kannst/willst/musst. Aber es gibt viele gute Gründe, warum (Teile von) einer Analyse nicht (nur) in einen dynamischen Bericht eingebettet werden sollten. Vielleicht bist du gerade dabei Daten zu bereinigen, um einen Datensatz für eine nachfolgende Analyse zu erzeugen. Vielleicht leistet du einen kleinen, aber entscheidenden Beitrag zu einem gigantischen Multi-Autoren-Papier. Etc. Denke auch daran, dass es natürlich auch noch andere Werkzeuge und Arbeitsabläufe gibt, um etwas reproduzierbar zu machen: z.B. [make]["minimal make: a minimal tutorial on make"].

## Load the tidyverse

Das Hauptpaket, das wir verwenden werden, ist [readr], welches Alternativen zu den Standardfunktionen `read.table()` und `write.table()` bietet. Trotzdem laden wir standardmäßig einfach wieder [tidyverse].

```{r start_import_export}
library(tidyverse)
```

## Einlesen der Gapminder Daten

Die Gapminder Daten könnten wir natürlich wie zuvor über das Laden des `gapminder` Pakets verfügbar machen. Da es in diesem Abschnitt aber um das Einlesen von Daten geht, versuchen wir die Daten als `.tsv` Datei (tab-separated values - so sind sie im Paket gespeichert) einzulesen. Aber dies bedeutet natürlich, dass wir die entsprechende `.tsv` Datei erst mal finden müssen. Dabei hilft uns glücklicherweise das [fs] Paket.

```{r}
library(fs)
(gap_tsv <- path_package("gapminder", "extdata", "gapminder.tsv"))
```

Nachdem wir jetzt den Speicherort der Datei kennen, können wir versuchen sie einzulesen.


## Einlesen von Daten in Tabellenform

Die Haupt-Funktion zum Einlesen von Daten in readr ist `read_delim()`. Hier verwenden wir eine Variante, `read_tsv()`, für tabulatorgetrennte Daten:

```{r}
gapminder <- read_tsv(gap_tsv)
glimpse(gapminder)
```


Über den Tabulator Spalten in einer Datentabelle zu trennen, ist natürlich nur eine Variante neben weiteren Alternativen wie Komma, Strichpunkt, Leerzeichen, ...

Für Komma getrennte Daten würde man beispielsweise `read_csv()` verwenden. Für volle Flexibilität bei der Angabe des Trennzeichens kannst du aber jederzeit direkt `read_delim()` verwenden.



Der auffälligste Unterschied zwischen den readr-Funktionen und der Standardfunktion `read.table()`ist, dass readr standardmäßig Characters NICHT in Faktoren umwandelt. Im Großen und Ganzen ist dies ein besseres Standardverhalten, obwohl es natürlich immer wieder vorkommen wird, dass du einzelne Variablen nach dem Einlesen in einen Faktoren umwandeln wirst. Aber lass dich davon nicht täuschen - im Allgemeinen wirst du durch die Verwendung von readr nach dem Einlesen  weniger Anpassungen machen müssen im Vergleich zum Standardvorgehen.

> Fazit: Benutze `readr::read_delim()` und "Freunde".


Die Gapminder-Daten sind zu sauber und einfach, um die großartigen Funktionen von readr zur Geltung zu bringen. Ein Blick in [Introduction to readr](https://cloud.r-project.org/web/packages/readr/vignettes/readr.html) zeigt aber noch viele weitere Anpassungsmöglichkeiten der readr Funktionen.

## Daten exportieren 

Bevor wir etwas exportieren können, müssen (das ist natürlich so nicht richtig - niemand zwingt uns dazu) etwas berechnen, das es wert ist,  exportiert zu werden. Lass uns eine Zusammenfassung der maximalen Lebenserwartung auf Länderebene erstellen.

```{r}
gap_life_exp <- gapminder %>%
  group_by(country, continent) %>% 
  summarise(life_exp = max(lifeExp)) %>% 
  ungroup()
gap_life_exp
```

Das `gap_life_exp` data frame ist ein Beispiel für ein Zwischenergebnis, das wir für die Zukunft und für nachgelagerte Analysen oder Visualisierungen speichern wollen.

Die Haupt-Exportfunktion in readr ist `write_delim()`. Für verschiedene Dateiformate gibt es auch hier wieder verschiedene Komfortfunktionen. Lass uns `write_csv()` benutzen, um eine kommagetrennte Datei zu erhalten.

```{r}
write_csv(gap_life_exp, "gap_life_exp.csv")
```

Schauen wir uns die ersten paar Zeilen von `gap_life_exp.csv` an. Dazu kannst du entweder die Datei öffnen oder, im Terminal, `head` darauf anwenden.

```{r echo = FALSE, comment = NA}
"gap_life_exp.csv" %>%
  readLines(n = 6) %>% 
  cat(sep = "\n")
```

Das sieht recht ordentlich aus, obwohl es keine sichtbare Ausrichtung oder Trennung in Spalten gibt. Hätten wir die Basisfunktion `read.csv()` benutzt, würden wir Zeilennamen und viele Anführungszeichen sehen, es sei denn, wir hätten diese Features explizit abgeschaltet. Das schönere Standardverhalten ist daher der Hauptgrund, warum wir `readr::write_csv()` gegenüber `write.csv()` bevorzugen.

> Es ist nicht wirklich fair, sich über den Mangel an sichtbarer Ausrichtung zu beklagen, schließlich erzeugen wir Dateien, die der Computer lesen soll. Falls du wirklich in der Datei "herumstöbern" willst, benutze  `View()` in RStudio oder öffnen die Datei mit einem Spreadsheet Programm (!). Aber erliege NIE der Versuchung, dort Datenmanipulationen vorzunehmen ... gehe  zurück zu R und schreibe dort die Befehle, die du die nächsten 15 Mal ausführen kannst, wenn du diesen Datensatz (oder Datensätze derselben Form) importieren/bereinigen/aggregieren/exportieren willst. 


## Daten über eine API

Interessante Datensätze sind der Treibstoff für ein gutes Data Science Projekt. APIs (Application Programming Interface) sind eine weitere sehr nützliche Methode, um auf interessante Daten zuzugreifen.

Anstatt einen Datensatz herunterladen zu müssen, ermöglichen APIs Daten direkt von bestimmten Websites über eine Schnittstelle anzufordern. Viele große Webseiten wie Twitter und Facebook ermöglichen über APIs den Zugriff auf Teile ihrer Daten.

Wir werden die Grundlagen des Zugriffs auf eine API besprechen. Dazu benötigst du aber keine Vorwissen bzgl. APIs.

### Einführung

API ist ein allgemeiner Begriff für den Ort, an dem ein Computerprogramm mit einem anderen oder mit sich selbst interagiert. Wir sprechen über Web-APIs, bei denen zwei verschiedene Computer - ein Client und ein Server - miteinander interagieren, um Daten anzufordern bzw. bereitzustellen.

APIs bieten eine ausgefeilte Möglichkeit Daten von einer Website anzufordern. Wenn eine Website wie Twitter eine API einrichtet, richten sie im Wesentlichen einen Computer ein, der auf Datenanfragen wartet.

Sobald dieser Computer eine Datenanforderung empfängt, verarbeitet er die Daten selbst und sendet sie an den Computer, der sie angefordert hat. Unsere Aufgabe als Anforderer der Daten wird es sein R Code zu schreiben, der die Anforderung erstellt und dem Computer, auf dem die API läuft, mitteilt, was wir benötigen. Dieser Computer liest dann unseren Code, verarbeitet die Anfrage und gibt schön formatierte Daten zurück, die mithilfe existierender R Pakete verarbeitet werden können..


### Erstellen von API-Anforderungen in R

Um mit APIs in R zu arbeiten, müssen wir ein paar neue Pakete laden (und vorher natürlich installieren). Konkret werden wir mit den Paketen  `httr` und `jsonlite` arbeiten. Sie spielen bei der Einbindung der APIs unterschiedliche Rollen, aber beide sind unverzichtbar.

Vermutlich hast du die beiden Pakete bisher nicht installiert. Daher ist der erste Schritt die beiden Pakete zu installieren

```{r, eval=FALSE}
install.packages(c("httr", "jsonlite"))

```

und anschließend zu laden

```{r}
library(httr)
library(jsonlite)
```


### Unsere erste API-Anfrage stellen

Der erste Schritt, um Daten von einer API zu erhalten, ist die eigentliche Anfrage in R. Diese Anfrage wird an den Computer-Server geschickt, der über die API verfügt, und wenn alles reibungslos verläuft, wird er eine Antwort zurücksenden. 


Es gibt verschiedene Arten von Anfragen, die man an einen API-Server stellen kann. Diese Arten von Anfragen entsprechen verschiedenen Aktionen, die der Server ausführen soll.

Für unsere Zwecke fragen wir lediglich nach Daten, was einer GET-Anfrage entspricht. Andere Arten von Anfragen sind z.B. POST und PUT, aber diese sind für uns nicht von Interesse und daher brauchen wir uns darum nicht zu kümmern.

Um eine GET-Anfrage zu erstellen, müssen wir die `GET()` Funktion aus dem `httr` Paket verwenden. Die `GET()` Funktion benötigt als Input eine URL, die die Adresse des Servers angibt, an den die Anforderung gesendet werden soll.


Als Beispiel werden wir mit der Open Notify API arbeiten, die Daten zu verschiedenen NASA-Projekten enthält. Mithilfe der Open Notify API können wir uns über den Standort der Internationalen Raumstation informieren und erfahren, wie viele Personen sich derzeit im Weltraum aufhalten.

Wir beginnen damit, dass wir unsere Anfrage mit der `GET()` Funktion stellen und die URL der API angeben:

```{r}
jdata <- GET("http://api.open-notify.org/astros.json")
```

Die Ausgabe der Funktion `GET()` ist eine Liste, die alle Informationen enthält, die vom API-Server zurückgegeben werden. 


### `GET()` Ausgabe

Schauen wir uns einmal an, wie die Variable jdata in der R-Konsole aussieht:

```{r}
jdata
```

Als erstes fällt auf, dass die URL enthalten ist, an die die GET-Anfrage gesendet wurde. Außerdem sehen wir das Datum und die Uhrzeit, zu der die Anfrage gestellt wurde, sowie die Größe der Antwort.

Die Information `Content-Type` gibt uns eine Vorstellung davon, welche Form die Daten haben. Diese spezielle Antwort besagt, dass die Daten ein JSON-Format annehmen, womit auch klar ist warum wir das Paket `jsonlite` ebenfalls geladen haben.

Der Status verdient eine besondere Aufmerksamkeit. `Status` bezieht sich auf den Erfolg oder Misserfolg der API-Anfrage, und er wird in Form einer Zahl angegeben. Die zurückgegebene Nummer gibt Auskunft darüber, ob die Anfrage erfolgreich war oder nicht, und kann auch einige Gründe für einen möglichen Misserfolg nennen.

Die Zahl 200 ist das, was wir sehen wollen. Sie entspricht einem erfolgreichen Antrag, und das ist es, was wir hier haben. Eine Übersicht über weitere Status Codes findet man z.B. auf dieser [Webseite](https://www.restapitutorial.com/httpstatuscodes.html.


### Handling JSON Data

[JSON](https://www.json.org/json-en.html) steht für JavaScript Object Notation. Während JavaScript eine weitere Programmiersprache ist, liegt unser Schwerpunkt bei JSON auf seiner Struktur. JSON ist nützlich, weil es von einem Computer leicht lesbar ist, und aus diesem Grund ist es zur primären Art und Weise geworden, wie Daten über APIs transportiert werden. Die meisten APIs senden ihre Antworten im JSON-Format.

JSON ist als eine Reihe von Schlüssel-Werte-Paaren formatiert, wobei ein bestimmtes Wort ("Schlüssel") mit einem bestimmten Wert assoziiert ist. Ein Beispiel für diese Schlüssel-Wert-Struktur ist unten dargestellt:

```
{
    “name”: “Jane Doe”,
    “number_of_skills”: 2
}
```

In ihrem aktuellen Zustand sind die Daten in der Variablen `jdata` nicht verwendbar. Die Daten sind als Unicode-Rohdaten in `jdata` enthalten, und müssen in das JSON-Format konvertiert werden.

Dazu müssen wir zunächst den rohen Unicode in Character Daten konvertieren, die dem oben gezeigten JSON-Format ähneln. Die Funktion `rawToChar()` führt genau diese Aufgabe aus:


```{r}
rawToChar(jdata$content)
```


Die resultierende Zeichenfolge sieht zwar recht unordentlich aus, aber es liegt wirklich die JSON-Struktur vor.

Ausgehend von diesem Character Vektor können wir nun mit `fromJSON()` aus dem `jsonlite` alles in ein Listenformat transformieren.

Die `fromJSON()` Funktion benötigt einen Character Vektor, der die JSON-Struktur enthält, die wir aus der Ausgabe von `rawToChar()` erhalten haben. Wenn wir also diese beiden Funktionen aneinanderreihen, erhalten wir die gewünschten Daten in einem Format, das wir in R leichter bearbeiten können.

```{r}
data <-  fromJSON(rawToChar(jdata$content))
glimpse(data)
```

Die Liste `data` hat drei Elemente. Uns interessiert in erster Linie das Data Frame `people`.

```{r}
data$people

```


Also, da haben wir unsere Antwort: Zum Zeitpunkt des letzten Updates `r date()
` von R4ews befanden sich `r nrow(data$people)` Personen im Weltraum. Aber wenn du alles selbst ausprobierst, könnten es auch schon wieder andere Namen und eine andere Anzahl sein. Das ist einer der Vorteile von APIs - im Gegensatz zu herunterladbaren Datensätzen werden sie im Allgemeinen in Echtzeit oder nahezu in Echtzeit aktualisiert, so dass sie eine großartige Möglichkeit darstellen, Zugang zu sehr aktuellen Daten zu erhalten.


In diesem Beispiel haben wir einen sehr unkomplizierten API-Workflow durchlaufen. Die meisten APIs erfordern, dass Sie demselben allgemeinen Muster folgen, aber dabei können sie durchaus komplexer sein.

In unserem Beispiel war es ausreichen nur die URL anzugeben. Aber einige APIs verlangen durchaus mehr Informationen vom Benutzer. Im letzten Teil dieser Einführung gehen wir darauf ein, wie du der API mit deiner Anfrage zusätzliche Informationen zur Verfügung stellen kannst.

### APIs und Abfrageparameter

Was wäre, wenn wir wissen wollten, wann die ISS einen bestimmten Ort auf der Erde überfliegen würde? Die ISS Pass Times API von Open Notify verlangt von uns, dass wir zusätzliche Parameter angeben, bevor sie die gewünschten Daten zurückgeben kann.

Wir müssen den Längen- und Breitengrad des Ortes angeben, nach dem wir im Rahmen unserer `GET()` Anfrage fragen. Sobald ein Längen- und Breitengrad angegeben ist, werden sie als Abfrageparameter mit der ursprünglichen URL kombiniert.

Lass uns die API verwenden, um herauszufinden, wann die ISS Garching (auf 48.24896 Breiten- und  11.65101 Längengrad) passieren wird:

```{r}
jdata <-  GET("http://api.open-notify.org/iss-pass.json",
    query = list(lat = 48.24896, lon = 11.65101))
```


Man muss in der Dokumentation für die API, mit man arbeiten will, nachsehen, ob es erforderliche Abfrageparameter gibt. Für die überwiegende Mehrheit der APIs, auf die du möglicherweise zugreifen möchtest, gibt es eine Dokumentation, die du lesen kannst (und lesen solltest), um ein klares Verständnis dafür zu erhalten, welche Parameter deine Anfrage erfordert. 

Wie auch immer, jetzt, da wir unsere Anfrage einschließlich der Standortparameter gestellt haben, können wir die Antwort mit den gleichen Funktionen überprüfen, die wir zuvor verwendet haben. Lass uns die Daten aus der Antwort extrahieren:


```{r}
data <- fromJSON(rawToChar(jdata$content))
data$response
```



Diese API gibt uns Zeiten in Form von [Unixzeit](https://de.wikipedia.org/wiki/Unixzeit) zurück. Unixzeit ist die Zeitspanne, die seit dem 1. Januar 1970 vergangen ist. Mithilfe der Funktion `as_datetime()` aus dem [lubridate] Paket können wir die Unixzeit aber leicht umrechnen

```{r}
lubridate::as_datetime(data$response$risetime)
```


Wir haben hier wirklich nur die Basics in Bezug auf APIs eingeführt. Aber hoffentlich hat dir diese Einführung trotzdem das Vertrauen gegeben, sich mit einigen komplexeren und leistungsfähigeren APIs auseinanderzusetzen, und trägt dadurch dazu bei, eine ganz neue Welt von Daten zu erschließen, die du erforschen kannst!



## Weiteres Material

Hier sein noch auf das Kapitel [Data import](http://r4ds.had.co.nz/data-import.html) im Buch [R for Data Science] von Hadley Wickham und Garrett Grolemund [-@wickham2016] verwiesen für weitere Information zum Daten Import.





```{r links, child="links.md"}

```

