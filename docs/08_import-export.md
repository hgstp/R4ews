# Daten I/O {#import-export}




## Ãœberblick

Im letzten Abschnitt haben wir die Gapminder-Daten als tibble aus dem [gapminder] Paket geladen. Dabei haben wir dann weder Daten, noch abgeleitete Ergebnisse, explizit in eine Datei geschrieben. Im wirklichen Leben werdet ihr aber stÃ¤ndig Daten, die in Tabellenform vorliegen, in R ein- und auslesen. Manchmal muss das sogar fÃ¼r Daten geschehen, die nicht in Tabellenform vorliegen.

> Wie macht man das? Worauf muss man aufpassen?

### Daten Import

FÃ¼r das Importieren von Daten gibt es im Allgemeinen zwei Szenarien:

* *"Ãœberrasche mich!"* Diese Haltung mÃ¼sst ihr einnehmen, wenn ihr einen Datensatz erhaltet und zum ersten Mal versucht diesen einzulesen. Man muss froh sein, wenn man die Daten ohne Fehlermeldung importieren kann. Im nÃ¤chsten Schritt schaut man sich  das Ergebnis an und  entdeckt vermutlich Fehler in den Daten und/oder beim Import. AnschlieÃŸend behebt man die Fehler und beginnt nochmal von vorne.

* *"Ein weiterer Tag im Paradies. "* Das wird vermutlich euer GefÃ¼hl sein, wenn ihr versucht einen [aufgerÃ¤umten Datensatz](#tidy) einzulesen (den jemand vorher in einem oder mehreren "Reinigungsskripten"  aufgerÃ¤umt hat). Beim Einlesen solcher Daten sollte es keine Ãœberraschungen geben. 

  
Im zweiten Fall, und im weiteren Verlauf des ersten Falles, lernt ihr tatsÃ¤chlich eine Menge darÃ¼ber, wie die Daten strukturiert sind/sein sollten. 

:::: {.content-box-orange}
__Ein wichtiger Import-Ratschlag:__ _Verwende die Argumente der Importfunktion, um so weit wie mÃ¶glich und so schnell wie mÃ¶glich zu kommen_. Macht man dies nicht, so sind oft nach dem Einlesen der Daten noch eine Reihe von weiteren Schritten nÃ¶tig, bevor man mit der eigentlichen Analyse beginnen kann. Daher lest die Hilfe zu den Importfunktionen und nutzt die Argumente maximal aus, um den Import zu steuern.
::::

### Daten Export

Es wird viele Gelegenheiten geben, bei denen ihr Daten aus R exportieren wollt. Zwei wichtige Beispiele:

* einen gesÃ¤uberten Datensatz, der bereit ist, analysiert zu werden

* ein numerisches Ergebnis aus einer Datenaggregation oder Modellierung oder einer statistischen Schlussfolgerung 


:::: {.content-box-gray}
__Erster Tipp:__  _Der Output von heute ist der Input von morgen_. Denkt an all die Schmerzen zurÃ¼ck, die ihr selbst beim Import von fremden Daten  erlitten habt, und fÃ¼gt euch nicht selbst solche Schmerzen zu!

__Zweiter Tipp:__ Seid nicht zu clever. Eine einfache Textdatei, die von einem Menschen in einem Texteditor lesbar ist, sollte euer _Standard_ sein, bis es _einen guten Grund_ dafÃ¼r gibt, dass dies nicht ausreichend ist. Das Lesen und Schreiben in exotische Formate wird das Erste sein, was mÃ¶glicherweise in Zukunft oder auf einem anderen Computer nicht mehr funktioniert. Zudem schafft es Barrieren fÃ¼r jeden, der ein anderes Toolkit hat als ihr. 
::::

Wie passt das zu unserer Betonung der dynamischen Berichterstattung Ã¼ber R Markdown? Es gibt fÃ¼r alles eine Zeit und einen Ort. Es gibt Projekte und Dokumente, bei denen ihr euch intensiv mit [knitr] und [rmarkdown]  beschÃ¤ftigen kÃ¶nnt/wollt/mÃ¼sst. Aber es gibt viele gute GrÃ¼nde, warum (Teile) einer Analyse nicht (nur) in einen dynamischen Bericht eingebettet werden sollten. Vielleicht wollt ihr Daten bereinigen, um einen Datensatz fÃ¼r eine nachfolgende Analyse zu erzeugen. Vielleicht leistet ihr einen kleinen, aber entscheidenden Beitrag zu einem gigantischen Multi-Autoren-Papier, usw. .... 

Denkt zudem daran, dass es natÃ¼rlich auch noch andere Werkzeuge und ArbeitsablÃ¤ufe gibt, um etwas reproduzierbar zu machen: z.B. [make]["minimal make: a minimal tutorial on make"].

## `readr`

Zur Einlesen und Ausgeben von DatensÃ¤tzen verwenden wir das [readr] Paket, welches Alternativen zu den Standardfunktionen `read.table()` und `write.table()` bietet. `readr` ist Teil des [tidyverse] und daher fÃ¼hren wir standardmÃ¤ÃŸig einfach wieder 


``` r
library(tidyverse)
## â”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€
## âœ” dplyr     1.1.4     âœ” readr     2.1.5
## âœ” forcats   1.0.0     âœ” stringr   1.5.1
## âœ” ggplot2   3.5.1     âœ” tibble    3.2.1
## âœ” lubridate 1.9.3     âœ” tidyr     1.3.1
## âœ” purrr     1.0.2     
## â”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€
## âœ– dplyr::filter() masks stats::filter()
## âœ– dplyr::lag()    masks stats::lag()
## â„¹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors
```

aus.

__Einlesen der Gapminder Daten__

Die Gapminder Daten kÃ¶nnten wir natÃ¼rlich wie zuvor Ã¼ber das Laden des `gapminder` Pakets verfÃ¼gbar machen. Da es in diesem Abschnitt aber um das Einlesen von Daten geht, versuchen wir die Daten als `.tsv` Datei (Tab getrennte Werte - so sind die Daten im Paket gespeichert) einzulesen. Aber dies bedeutet natÃ¼rlich, dass wir die entsprechende `.tsv` Datei erst mal finden mÃ¼ssen. Dabei hilft uns glÃ¼cklicherweise das [fs] Paket.


``` r
library(fs)
(gap_tsv <- path_package("gapminder", "extdata", "gapminder.tsv"))
## /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/gapminder/extdata/gapminder.tsv
```

Nachdem wir jetzt den Speicherort der Datei kennen, kÃ¶nnen wir versuchen sie einzulesen.


## Einlesen von Daten in Tabellenform

Die Haupt-Funktion zum Einlesen von Daten in readr ist `read_delim()`. Hier verwenden wir eine Variante, `read_tsv()`, fÃ¼r Tab getrennte Daten:


``` r
gapminder <- read_tsv(gap_tsv)
## Rows: 1704 Columns: 6
## â”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
## Delimiter: "\t"
## chr (2): country, continent
## dbl (4): year, lifeExp, pop, gdpPercap
## 
## â„¹ Use `spec()` to retrieve the full column specification for this data.
## â„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
gapminder
## # A tibble: 1,704 Ã— 6
##    country     continent  year lifeExp      pop gdpPercap
##    <chr>       <chr>     <dbl>   <dbl>    <dbl>     <dbl>
##  1 Afghanistan Asia       1952    28.8  8425333      779.
##  2 Afghanistan Asia       1957    30.3  9240934      821.
##  3 Afghanistan Asia       1962    32.0 10267083      853.
##  4 Afghanistan Asia       1967    34.0 11537966      836.
##  5 Afghanistan Asia       1972    36.1 13079460      740.
##  6 Afghanistan Asia       1977    38.4 14880372      786.
##  7 Afghanistan Asia       1982    39.9 12881816      978.
##  8 Afghanistan Asia       1987    40.8 13867957      852.
##  9 Afghanistan Asia       1992    41.7 16317921      649.
## 10 Afghanistan Asia       1997    41.8 22227415      635.
## # â„¹ 1,694 more rows
```


Wie wir sehen, wurde standardmÃ¤ÃŸig der komplette Datensatz eingelesen. Sind aber nur Teile eines Datensatzes relevant fÃ¼r die angestrebte Analyse, so besteht auch keine Notwendigkeit den kompletten Datensatz zu laden. In solchen FÃ¤llen kann man mit dem `col_types` Argument arbeiten


``` r
gapminder_short <- read_tsv(gap_tsv, col_types = cols_only(
  country = col_character(),
  continent = col_factor(),
  year = col_double(),
  lifeExp = col_double()
))
gapminder_short
## # A tibble: 1,704 Ã— 4
##    country     continent  year lifeExp
##    <chr>       <fct>     <dbl>   <dbl>
##  1 Afghanistan Asia       1952    28.8
##  2 Afghanistan Asia       1957    30.3
##  3 Afghanistan Asia       1962    32.0
##  4 Afghanistan Asia       1967    34.0
##  5 Afghanistan Asia       1972    36.1
##  6 Afghanistan Asia       1977    38.4
##  7 Afghanistan Asia       1982    39.9
##  8 Afghanistan Asia       1987    40.8
##  9 Afghanistan Asia       1992    41.7
## 10 Afghanistan Asia       1997    41.8
## # â„¹ 1,694 more rows
```

Zur Auswahl eines Teils der Variablen haben wir `cols_only()` verwendet. Diese Funktion erwartet bei der Auswahl der Variablen die Definition des Typs. In diesem Beispiel haben wir `continten` (anders als im Standardfall) zu einer [Faktorvariable](https://r4ds.had.co.nz/factors.html) transformiert. Dadurch enthÃ¤lt die Variable zusÃ¤tzlich die Information Ã¼ber die verschiedenen AusprÃ¤gungen der Variable


``` r
levels(gapminder_short$continent)
## [1] "Asia"     "Europe"   "Africa"   "Americas" "Oceania"
```





Ãœber den Tabulator Spalten in einer Datentabelle zu trennen, ist natÃ¼rlich nur eine MÃ¶glichkeit von vielen. Weitere Alternativen sind: 

+ Komma: `read_csv()`

+ Strichpunkt: `read_csv2()`

+ Leerzeichen: `read_table()`

+ ...


FÃ¼r volle FlexibilitÃ¤t bei der Angabe des Trennzeichens kann aber jederzeit direkt `read_delim()` verwendet werden.



Der auffÃ¤lligste Unterschied zwischen den [readr]-Funktionen und der Standardfunktion `read.table()`ist, dass [readr] immer ein Tibble erzeugt statt eines Data Frames. Da wir Tibbles bevorzugen ist unser 


> Fazit: Benutzt `readr::read_delim()` und "Freunde".


Die Gapminder-Daten sind zu sauber und einfach, um die groÃŸartigen Funktionen von readr zur Geltung zu bringen. Ein Blick in [Introduction to readr](https://cloud.r-project.org/web/packages/readr/vignettes/readr.html) zeigt aber noch viele weitere AnpassungsmÃ¶glichkeiten der readr Funktionen.

## Daten exportieren 

Bevor wir etwas exportieren kÃ¶nnen, mÃ¼ssen wir natÃ¼rlich (was so sicher nicht richtig ist - niemand zwingt uns dazu ðŸ˜‰) etwas berechnen, das es wert ist,  exportiert zu werden. Lasst uns daher eine Zusammenfassung der maximalen Lebenserwartung auf LÃ¤nderebene erstellen.


``` r
gap_life_exp <- gapminder |>  
  group_by(country, continent) |> 
  summarise(life_exp = max(lifeExp)) |> 
  ungroup()
## `summarise()` has grouped output by 'country'. You can override using the
## `.groups` argument.
gap_life_exp
## # A tibble: 142 Ã— 3
##    country     continent life_exp
##    <chr>       <chr>        <dbl>
##  1 Afghanistan Asia          43.8
##  2 Albania     Europe        76.4
##  3 Algeria     Africa        72.3
##  4 Angola      Africa        42.7
##  5 Argentina   Americas      75.3
##  6 Australia   Oceania       81.2
##  7 Austria     Europe        79.8
##  8 Bahrain     Asia          75.6
##  9 Bangladesh  Asia          64.1
## 10 Belgium     Europe        79.4
## # â„¹ 132 more rows
```

Das Objekt `gap_life_exp` betrachten wir nun als Zwischenergebnis, das wir fÃ¼r die Zukunft und fÃ¼r nachgelagerte Analysen oder Visualisierungen speichern wollen.

Die Haupt-Exportfunktion in [readr] ist `write_delim()`. FÃ¼r verschiedene Dateiformate gibt es auch hier wieder verschiedene Komfortfunktionen. Mithilfe von `write_csv()` kÃ¶nnen wir den Inhalt von `gap_life_exp` in einer kommagetrennten Datei abspeichern.



``` r
write_csv(gap_life_exp, "data/gap_life_exp.csv")
```

Schauen wir uns die ersten paar Zeilen von `gap_life_exp.csv` an. Dazu kÃ¶nnen wir entweder die Datei Ã¶ffnen oder z.B. im Terminal  `head` verwenden



<img src="img/head_csv.png" width="100%" style="display: block; margin: auto;" />

Das sieht recht ordentlich aus, obwohl es keine sichtbare Ausrichtung oder Trennung in Spalten gibt. HÃ¤tten wir die Basisfunktion `read.csv()` benutzt, wÃ¼rden wir Zeilennamen und viele AnfÃ¼hrungszeichen sehen, es sei denn, wir hÃ¤tten diese Features explizit abgeschaltet. Das schÃ¶nere Standardverhalten ist daher der Hauptgrund, warum wir `readr::write_csv()` gegenÃ¼ber `write.csv()` bevorzugen.

:::: {.content-box-grey}
__Bemerkung:__ Es ist auch nicht wirklich fair, sich Ã¼ber den Mangel an sichtbarer Ausrichtung zu beklagen. SchlieÃŸlich erzeugen wir Dateien, die der Computer lesen soll. Falls ihr aber wirklich in der Datei "herumstÃ¶bern" wollt, benutzt  `View()` in RStudio. Oder Ã¶ffnet die Datei mit einem Spreadsheet Programm (!). Aber erliegt __NIE__ der Versuchung, dort Datenmanipulationen vorzunehmen ... kehrt zurÃ¼ck zu R und schreibt dort die entsprechenden Befehle, die ihr die nÃ¤chsten 15 Mal (oder so oft wie nÃ¶tig) ausfÃ¼hren kÃ¶nnt, wenn ihr diesen Datensatz (oder DatensÃ¤tze derselben Form) importieren/bereinigen/aggregieren/exportieren wollt. 
::::


## Daten Ã¼ber eine API

APIs (Application Programming Interface) sind eine sehr nÃ¼tzliche Methode, um auf interessante Daten zuzugreifen, die online zur VerfÃ¼gung gestellt werden.

Anstatt einen Datensatz herunterladen zu mÃ¼ssen, ermÃ¶glichen APIs Daten direkt von bestimmten Webseiten Ã¼ber eine Schnittstelle anzufordern. Viele groÃŸe Webseiten wie [Twitter](https://www.earthdatascience.org/courses/earth-analytics/get-data-using-apis/intro-to-social-media-text-mining-r/) und Facebook ermÃ¶glichen Ã¼ber APIs den Zugriff auf Teile ihrer Daten.

Wir werden die Grundlagen des Zugriffs auf eine API besprechen. Dazu benÃ¶tigt ihr aber kein Vorwissen bzgl. APIs.

### EinfÃ¼hrung

API ist ein allgemeiner Begriff fÃ¼r den Ort, an dem ein Computerprogramm mit einem anderen oder mit sich selbst interagiert. Wir sprechen Ã¼ber Web-APIs, bei denen zwei verschiedene Computer - ein Client und ein Server - miteinander interagieren, um Daten anzufordern bzw. bereitzustellen.

APIs bieten eine ausgefeilte MÃ¶glichkeit Daten von einer Webseite anzufordern. Wenn eine Webseite wie Twitter eine API einrichtet, richten sie im Wesentlichen einen Computer ein, der auf Datenanfragen wartet.

Sobald dieser Computer eine Datenabfrage empfÃ¤ngt, verarbeitet er die Daten selbst und sendet sie an den Computer, der sie angefordert hat. Unsere Aufgabe wird es sein R Code zu schreiben, der die Anfrage erstellt und dem Computer, auf dem die API lÃ¤uft, mitteilt, was wir benÃ¶tigen. Dieser Computer liest dann unseren Code, verarbeitet die Anfrage und gibt schÃ¶n formatierte Daten zurÃ¼ck, die mithilfe existierender R Pakete verarbeitet werden kÃ¶nnen.


### Erstellen von API-Abfragen in R

Um mit APIs in R zu arbeiten, mÃ¼ssen wir ein paar neue Pakete laden (und vorher natÃ¼rlich installieren). Konkret werden wir mit den Paketen  `httr` und `jsonlite` arbeiten. Sie spielen bei der Einbindung der APIs unterschiedliche Rollen, aber beide sind unverzichtbar.

Vermutlich habt ihr die beiden Pakete bisher nicht installiert. Daher starten wir mit dem Installieren dieser beiden Pakete


``` r
install.packages(c("httr", "jsonlite"))

```

und laden sie anschlieÃŸend 


``` r
library(httr)
library(jsonlite)
## 
## Attaching package: 'jsonlite'
## The following object is masked from 'package:purrr':
## 
##     flatten
```


### Unsere erste API-Anfrage stellen

Der erste Schritt, um Daten von einer API zu erhalten, ist die eigentliche Anfrage in R. Diese Anfrage wird an den Server geschickt, der Ã¼ber die API verfÃ¼gt, und wenn alles reibungslos verlÃ¤uft, wird er uns eine Antwort zurÃ¼cksenden. 


Es gibt verschiedene Arten von Anfragen, die man an einen API-Server stellen kann. Diese verschiedenen Typen von Anfragen entsprechen verschiedenen Aktionen, die der Server ausfÃ¼hren soll.

FÃ¼r unsere Zwecke fragen wir lediglich nach Daten, was einer __GET__-Anfrage entspricht. Andere Arten von Anfragen sind z.B. POST (post file) und PUT (send put request), aber diese sind fÃ¼r uns nicht von Interesse und werden wir daher nicht weiter besprechen.

Um eine GET-Anfrage zu erstellen, mÃ¼ssen wir die `GET()` Funktion aus dem `httr` Paket verwenden. Die `GET()` Funktion benÃ¶tigt als Input eine URL, die die Adresse des Servers angibt, an den die Anforderung gesendet werden soll.


Als Beispiel werden wir mit der [Open Notify](http://open-notify.org/) API arbeiten, die Daten zu verschiedenen NASA-Projekten enthÃ¤lt. Mithilfe der Open Notify API kÃ¶nnen wir uns Ã¼ber den Standort der Internationalen Raumstation informieren und erfahren, wie viele Personen sich derzeit im Weltraum aufhalten.

Wir beginnen damit, dass wir unsere Anfrage mit der `GET()` Funktion stellen und die URL der API angeben:


``` r
jdata <- GET("http://api.open-notify.org/astros.json")
```

Die Ausgabe der Funktion `GET()` ist eine Liste, die alle Informationen enthÃ¤lt, die vom API-Server zurÃ¼ckgegeben werden. 


### `GET()` Ausgabe

Schauen wir uns an, wie die Variable `jdata` in der R-Konsole aussieht:


``` r
jdata
## Response [http://api.open-notify.org/astros.json]
##   Date: 2024-11-27 21:38
##   Status: 200
##   Content-Type: application/json
##   Size: 587 B
```

Als erstes fÃ¤llt auf, dass die URL enthalten ist, an die die GET-Anfrage gesendet wurde. AuÃŸerdem erkennen wir das Datum und die Uhrzeit, zu der die Anfrage gestellt wurde, sowie die GrÃ¶ÃŸe der Antwort.

Die Information `Content-Type` gibt uns eine Vorstellung davon, welche Form die Daten haben. Diese spezielle Antwort besagt, dass die Daten ein JSON-Format annehmen, womit auch klar ist warum wir das Paket `jsonlite` geladen haben.

Der Status verdient eine besondere Aufmerksamkeit. `Status` bezieht sich auf den Erfolg oder Misserfolg der API-Anfrage, und er wird in Form einer Zahl angegeben. Die zurÃ¼ckgegebene Nummer gibt Auskunft darÃ¼ber, ob die Anfrage erfolgreich war oder nicht. Dort kÃ¶nnen auch GrÃ¼nde fÃ¼r einen mÃ¶glichen Misserfolg enthalten sein.

Die Zahl 200 ist das, was wir sehen wollen. Sie entspricht einem erfolgreichen Antrag, und das ist es, was wir hier haben. Eine Ãœbersicht Ã¼ber weitere Status Codes findet man z.B. auf dieser [Webseite](https://www.restapitutorial.com/httpstatuscodes.html).


### Handling JSON Data

[JSON](https://www.json.org/json-en.html) steht fÃ¼r JavaScript Object Notation. WÃ¤hrend JavaScript eine weitere Programmiersprache ist, liegt unser Schwerpunkt bei JSON auf seiner Struktur. JSON ist nÃ¼tzlich, weil es von einem Computer leicht lesbar ist, und aus diesem Grund ist es zur primÃ¤ren Art und Weise geworden, wie Daten Ã¼ber APIs transportiert werden. Die meisten APIs senden ihre Antworten im JSON-Format.

JSON ist als eine Reihe von SchlÃ¼ssel-Werte-Paaren formatiert, wobei ein bestimmtes Wort ("SchlÃ¼ssel") mit einem bestimmten Wert assoziiert ist. Ein Beispiel fÃ¼r diese SchlÃ¼ssel-Wert-Struktur ist unten dargestellt:

```
{
    â€œnameâ€: â€œJane Doeâ€,
    â€œnumber_of_skillsâ€: 2
}
```

In ihrem aktuellen Zustand sind die Daten in der Variablen `jdata` nicht verwendbar. Die Daten sind als Unicode-Rohdaten in `jdata` enthalten, und mÃ¼ssen in das JSON-Format konvertiert werden.

Dazu mÃ¼ssen wir zunÃ¤chst den rohen Unicode in character Daten konvertieren, die dem oben gezeigten JSON-Format Ã¤hneln. Die Funktion `rawToChar()` fÃ¼hrt genau diese Aufgabe aus:



``` r
rawToChar(jdata$content)
## [1] "{\"people\": [{\"craft\": \"ISS\", \"name\": \"Oleg Kononenko\"}, {\"craft\": \"ISS\", \"name\": \"Nikolai Chub\"}, {\"craft\": \"ISS\", \"name\": \"Tracy Caldwell Dyson\"}, {\"craft\": \"ISS\", \"name\": \"Matthew Dominick\"}, {\"craft\": \"ISS\", \"name\": \"Michael Barratt\"}, {\"craft\": \"ISS\", \"name\": \"Jeanette Epps\"}, {\"craft\": \"ISS\", \"name\": \"Alexander Grebenkin\"}, {\"craft\": \"ISS\", \"name\": \"Butch Wilmore\"}, {\"craft\": \"ISS\", \"name\": \"Sunita Williams\"}, {\"craft\": \"Tiangong\", \"name\": \"Li Guangsu\"}, {\"craft\": \"Tiangong\", \"name\": \"Li Cong\"}, {\"craft\": \"Tiangong\", \"name\": \"Ye Guangfu\"}], \"number\": 12, \"message\": \"success\"}"
```


Die resultierende Zeichenfolge sieht zwar recht unordentlich aus, aber es liegt wirklich die JSON-Struktur vor.

Ausgehend von diesem character Vektor kÃ¶nnen wir nun mit `fromJSON()`, aus dem `jsonlite` Paket, alles in ein Listenformat transformieren.

Die `fromJSON()` Funktion benÃ¶tigt einen character Vektor, der die JSON-Struktur enthÃ¤lt, die wir aus der Ausgabe von `rawToChar()` erhalten haben. Wenn wir also diese beiden Funktionen nacheinander anwenden, erhalten wir die gewÃ¼nschten Daten in einem Format, das wir in R leicht bearbeiten kÃ¶nnen.


``` r
data <-  fromJSON(rawToChar(jdata$content))
glimpse(data)
## List of 3
##  $ people :'data.frame':	12 obs. of  2 variables:
##   ..$ craft: chr [1:12] "ISS" "ISS" "ISS" "ISS" ...
##   ..$ name : chr [1:12] "Oleg Kononenko" "Nikolai Chub" "Tracy Caldwell Dyson"..
##  $ number : int 12
##  $ message: chr "success"
```

Die Liste `data` hat drei Elemente. Uns interessiert in erster Linie das Data Frame `people`.


``` r
data$people
##       craft                 name
## 1       ISS       Oleg Kononenko
## 2       ISS         Nikolai Chub
## 3       ISS Tracy Caldwell Dyson
## 4       ISS     Matthew Dominick
## 5       ISS      Michael Barratt
## 6       ISS        Jeanette Epps
## 7       ISS  Alexander Grebenkin
## 8       ISS        Butch Wilmore
## 9       ISS      Sunita Williams
## 10 Tiangong           Li Guangsu
## 11 Tiangong              Li Cong
## 12 Tiangong           Ye Guangfu
```


Also, da haben wir unsere Antwort: Zum Zeitpunkt des letzten Updates Nov 27, 2024 von R4ews befanden sich 12 Personen im Weltraum. Aber wenn ihr den Code zu einem spÃ¤teren Zeitpunkt ausprobiert, kÃ¶nnten es auch schon wieder andere Namen und eine andere Anzahl sein. Das ist einer der Vorteile von APIs - im Gegensatz zu DatensÃ¤tzen, die man im Spreadsheet Format herunterladen kann, werden sie in der Regel in Echtzeit oder nahezu in Echtzeit aktualisiert. APIs bieten somit die MÃ¶glichkeit leicht auf sehr aktuelle Daten zuzugreifen.


In diesem Beispiel haben wir einen sehr unkomplizierten API-Workflow durchlaufen. Die meisten APIs fordern, dass man demselben allgemeinen Muster folgt, aber dabei kÃ¶nnen die jeweilgen Aufrufe/Befehle durchaus deutlich komplexer sein.

In unserem Beispiel war es ausreichend nur die URL anzugeben. Aber einige APIs verlangen mehr Informationen vom Benutzer. Darauf gehen wir aber erstmal nicht weiter ein. Stattdessen fragen wir noch nach dem Ort der ISS im Moment der Abfrage


``` r
jdata <-  GET("http://api.open-notify.org/iss-now.json",)
```





``` r
data <- fromJSON(rawToChar(jdata$content))
data$iss_position
## $latitude
## [1] "-28.6648"
## 
## $longitude
## [1] "166.8485"
data$timestamp
## [1] 1732743482
```



Diese API gibt uns die Zeit in Form von [Unixzeit](https://de.wikipedia.org/wiki/Unixzeit) zurÃ¼ck. Unixzeit ist die Zeitspanne, die seit dem 1. Januar 1970 vergangen ist. Mithilfe der Funktion `as_datetime()` aus dem [lubridate] Paket kÃ¶nnen wir die Unixzeit aber leicht umrechnen


``` r
lubridate::as_datetime(data$timestamp)
## [1] "2024-11-27 21:38:02 UTC"
```



Damit wollen wir den Abschnitt zu APIs beenden. Macht euch bewusst, dass wir hier wirklich nur die Basics in Bezug auf APIs eingefÃ¼hrt haben. Aber hoffentlich hat euch diese EinfÃ¼hrung trotzdem ausreichend Vertrauen gegeben, sich mit einigen komplexeren und leistungsfÃ¤higeren APIs auseinanderzusetzen. 


## Weiteres Material

Wer noch mehr zum Thema Daten Import lesen will, der soll einen Blick in das Kapitel [Data import](http://r4ds.had.co.nz/data-import.html) im Buch [R for Data Science] von Hadley Wickham und Garrett Grolemund [-@wickham2016] werfen.







<!--Packages: main link-->
[dplyr]: https://dplyr.tidyverse.org
[tidyr]: https://tidyr.tidyverse.org
[ggplot2]: https://ggplot2.tidyverse.org
[tidyverse]: https://tidyverse.tidyverse.org
[stringr]: https://stringr.tidyverse.org
[forcats]: https://forcats.tidyverse.org
[purrr]: https://purrr.tidyverse.org
[readr]: https://readr.tidyverse.org
[fs]: https://fs.r-lib.org/index.html
[glue]: https://glue.tidyverse.org
[testthat]: https://testthat.r-lib.org
[ellipsis]: https://ellipsis.r-lib.org
[lubridate]: https://lubridate.tidyverse.org
[devtools]: https://devtools.r-lib.org
[roxygen2]: https://roxygen2.r-lib.org
[knitr]: https://github.com/yihui/knitr
[rmarkdown]: https://rmarkdown.rstudio.com/
[usethis]: https://usethis.r-lib.org
[xml2]: https://xml2.r-lib.org
[httr]: https://httr.r-lib.org
[rvest]: https://rvest.tidyverse.org
[Shiny]: https://shiny.rstudio.com
[gh]: https://github.com/r-lib/gh
[plyr]: http://plyr.had.co.nz
[magrittr]: https://magrittr.tidyverse.org
[googlesheets]: https://github.com/jennybc/googlesheets
[gapminder]: https://github.com/jennybc/gapminder
[stringi]: http://www.gagolewski.com/software/stringi/
[rex]: https://github.com/kevinushey/rex
[lattice]: http://lattice.r-forge.r-project.org
[RColorBrewer]: https://cloud.r-project.org/package=RColorBrewer
[gridExtra]: https://cloud.r-project.org/package=gridExtra
[rebird]: https://docs.ropensci.org/rebird/
[geonames]: https://docs.ropensci.org/geonames/
[rplos]: https://docs.ropensci.org/rplos/
[gender]: https://docs.ropensci.org/gender/
[genderdata]: https://docs.ropensci.org/genderdata/
[curl]: https://jeroen.cran.dev/curl
[jsonlite]: https://github.com/jeroen/jsonlite
[shinythemes]: https://rstudio.github.io/shinythemes/
[shinyjs]: https://deanattali.com/shinyjs/
[leaflet]: https://rstudio.github.io/leaflet/
[ggvis]: https://ggvis.rstudio.com
[shinydashboard]: https://rstudio.github.io/shinydashboard/

<!--Packages: vignettes & CRAN/GitHub links-->
[Introduction to dplyr]: https://dplyr.tidyverse.org/articles/dplyr.html
[Window functions]: https://dplyr.tidyverse.org/articles/window-functions.html
[Two-table verbs]: https://dplyr.tidyverse.org/articles/two-table.html
[Do more with dates and times in R]: https://lubridate.tidyverse.org/articles/lubridate.html
[dplyr-cran]: https://cloud.r-project.org/package=dplyr
[dplyr-github]: https://github.com/hadley/dplyr

<!--Bookdowns: main link-->
[Happy Git and GitHub for the useR]: https://happygitwithr.com
[R for Data Science]: https://r4ds.had.co.nz
[The tidyverse style guide]: https://style.tidyverse.org
[Advanced R]: https://adv-r.hadley.nz/
[Tidyverse design principles]: https://principles.tidyverse.org
[R Packages]: https://r-pkgs.org/index.html
[R Graphics Cookbook]: http://shop.oreilly.com/product/0636920023135.do
[Cookbook for R]: http://www.cookbook-r.com 
[ggplot2: Elegant Graphics for Data Analysis]: https://ggplot2-book.org/index.html
[Statistical Inference via Data Science]: https://moderndive.com/index.html

<!--Bookdowns: specific chapters-->
[adv-r-fxn-args]: http://adv-r.had.co.nz/Functions.html#function-arguments
[r4ds-transform]: https://r4ds.had.co.nz/transform.html
[r4ds-readr-strings]: https://r4ds.had.co.nz/data-import.html#readr-strings

<!--RStudio Cheat Sheets--> 
[RStudio Data Transformation Cheat Sheet]: https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf
[Regular Expressions in R Cheat Sheet]: https://github.com/rstudio/cheatsheets/raw/master/regex.pdf
[Shiny Cheat Sheet]: https://shiny.rstudio.com/articles/cheatsheet.html

<!--Blog posts, slides, & papers-->
["minimal make: a minimal tutorial on make"]: https://kbroman.org/minimal_make/
["Let the Data Flow: Pipelines in R with dplyr and magrittr"]: https://github.com/tjmahr/MadR_Pipelines
["Hands-on dplyr tutorial for faster data manipulation in R"]: https://www.dataschool.io/dplyr-tutorial-for-faster-data-manipulation-in-r/
["Writing R Extensions"]: https://cloud.r-project.org/doc/manuals/r-release/R-exts.html
["The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!)"]: https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/
["What Every Programmer Absolutely, Positively Needs To Know About Encodings And Character Sets To Work With Text"]: http://kunststube.net/encoding/
["3 Steps to Fix Encoding Problems in Ruby"]: https://www.justinweiss.com/articles/3-steps-to-fix-encoding-problems-in-ruby/
["My favorite RGB color"]: https://manyworldstheory.com/2013/01/15/my-favorite-rgb-color/

<!--Papers/Books Cited-->
["Dates and Times Made Easy with lubridate"]: https://www.jstatsoft.org/article/view/v040i03
["testthat: Get Started with Testing"]: https://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf
["Let's Practice What We Preach"]: https://www.jstor.org/stable/3087382?seq=1#page_scan_tab_contents
[Creating More Effective Graphs]: https://www.amazon.com/Creating-Effective-Graphs-Naomi-Robbins/dp/0985911123
["Escaping RGBland: Selecting Colors for Statistical Graphs"]: https://eeecon.uibk.ac.at/~zeileis/papers/Zeileis+Hornik+Murrell-2009.pdf
["A layered grammar of graphics"]: https://vita.had.co.nz/papers/layered-grammar.html
[Managing Projects with GNU Make, 3rd Edition]: http://shop.oreilly.com/product/9780596006105.do
["Why Should Engineers and Scientists Be Worried About Color?"]: https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=2ahUKEwi0xYqJ8JbjAhWNvp4KHViYDxsQFjABegQIABAC&url=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FAhmed_Elhattab2%2Fpost%2FPlease_suggest_some_good_3D_plot_tool_Software_for_surface_plot%2Fattachment%2F5c05ba35cfe4a7645506948e%2FAS%253A699894335557644%25401543879221725%2Fdownload%2FWhy%2BShould%2BEngineers%2Band%2BScientists%2BBe%2BWorried%2BAbout%2BColor_.pdf&usg=AOvVaw1qwjjGMd7h_z6TLUjzu7Nb

<!--Misc.-->
[rOpenSci]: https://ropensci.org

[wiki-snake-case]: https://en.wikipedia.org/wiki/Snake_case




